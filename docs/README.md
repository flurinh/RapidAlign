# RapidAlign Documentation

![RapidAlign Logo](images/logo.png)

## Table of Contents

1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Core Features](#core-features)
4. [API Reference](#api-reference)
5. [Performance Optimizations](#performance-optimizations)
6. [Advanced Usage](#advanced-usage)
7. [Benchmarks](#benchmarks)
8. [Integration Examples](#integration-examples)
9. [Troubleshooting](#troubleshooting)
10. [Contributing](#contributing)

## Introduction

RapidAlign is a high-performance CUDA-accelerated library for fast batch point cloud and graph alignment with deep learning integration. It provides optimized implementations of point cloud alignment algorithms (Procrustes, ICP, Chamfer) designed specifically for geometric deep learning and 3D computer vision workflows.

### Key Features

- **CUDA-Accelerated**: Leverages GPU compute for fast alignment operations
- **Batched Processing**: Handle multiple point clouds with variable sizes in a single operation
- **Grid-Based Acceleration**: Spatial partitioning for O(n) nearest neighbor search
- **CUDA Streams**: Concurrent processing for optimal GPU utilization
- **PyTorch Integration**: Seamless integration with PyTorch and PyTorch Geometric
- **Production-Ready**: Comprehensive benchmarking and testing tools

### Use Cases

- **Geometric Deep Learning**: Align graph structures in 3D space during GNN training
- **Point Cloud Registration**: Fast registration of multiple 3D scans
- **Robot Perception**: Real-time alignment for robotics applications
- **3D Data Processing**: Batch operations on large point cloud datasets

## Installation

### Prerequisites

- CUDA Toolkit (10.2 or newer)
- PyTorch (1.7 or newer)
- C++ compiler compatible with your CUDA version

### Basic Installation

```bash
pip install rapidalign
```

### Installing from Source

```bash
git clone https://github.com/username/rapidalign.git
cd rapidalign/python
pip install -e .
```

### Installation with Optional Dependencies

```bash
# For visualization tools
pip install rapidalign[visualize]

# For PyTorch Geometric integration
pip install rapidalign[pyg]

# For all optional dependencies
pip install rapidalign[all]
```

### CUDA Version Compatibility

RapidAlign is designed to work with different CUDA versions, including CUDA version mismatches between the system and PyTorch. It will automatically detect your CUDA version and configure itself appropriately.

If you encounter CUDA version mismatch errors, you can force compilation with:

```bash
TORCH_ALLOW_CUDA_VERSION_MISMATCH=1 pip install -e .
```

## Core Features

### Batched Procrustes Analysis

The Procrustes algorithm computes the optimal rigid transformation (rotation and translation) between two point clouds with known correspondences. RapidAlign's implementation is batched, meaning it can process multiple point cloud pairs simultaneously.

```python
import torch
from rapidalign import BatchedProcrustes

# Create point clouds
src_points = torch.rand(1000, 3).cuda()  # Source points
tgt_points = torch.rand(1000, 3).cuda()  # Target points

# Create Procrustes module
procrustes = BatchedProcrustes()

# Align points
aligned_points, (rotations, translations) = procrustes(src_points, tgt_points)
```

### Batched Iterative Closest Point (ICP)

The ICP algorithm aligns point clouds without known correspondences by iteratively finding nearest neighbors and solving the alignment problem. RapidAlign provides a fast, batched implementation with multiple optimization strategies.

```python
from rapidalign import BatchedICP

# Create ICP module with optimizations
icp = BatchedICP(
    max_iterations=20,
    convergence_threshold=1e-6,
    use_grid_acceleration=True,
    use_cuda_streams=True,
    grid_cell_size=0.2
)

# Align point clouds
aligned_points, (rotations, translations) = icp(src_points, tgt_points)
```

### Batched Chamfer Distance

The Chamfer distance is a metric used to measure the similarity between two point clouds. RapidAlign provides a fast implementation that can be used for loss functions in neural networks.

```python
from rapidalign import BatchedChamferLoss

# Create Chamfer loss with grid acceleration
chamfer = BatchedChamferLoss(
    reduction='mean',
    use_grid_acceleration=True,
    grid_cell_size=0.2
)

# Compute distance
loss = chamfer(predicted_points, target_points)
```

## API Reference

### BatchedProcrustes

```python
class BatchedProcrustes(torch.nn.Module):
    """
    Differentiable Batched Procrustes Alignment for PyTorch
    
    This module aligns batches of source point clouds to target point clouds
    using the Procrustes algorithm. Supports point clouds with varying sizes.
    """
    
    def __init__(self):
        """Initialize the Procrustes alignment module."""
        super(BatchedProcrustes, self).__init__()
    
    def forward(self, src_points, tgt_points, batch_indices=None):
        """
        Aligns batches of source point clouds to target point clouds
        
        Parameters:
        -----------
        src_points : torch.Tensor 
            Source points with shape [N, 3] or batched points [B, N, 3]
        tgt_points : torch.Tensor
            Target points with shape [M, 3] or batched points [B, M, 3]
        batch_indices : torch.Tensor, optional
            Batch indices for each point, shape [N] for src and [M] for tgt
            Only needed if not using batched points format
            
        Returns:
        --------
        aligned_points : torch.Tensor
            Aligned source points with same shape as src_points
        transforms : tuple(torch.Tensor, torch.Tensor)
            (rotation_matrices, translation_vectors)
            rotation_matrices: shape [B, 3, 3]
            translation_vectors: shape [B, 3]
        """
```

### BatchedICP

```python
class BatchedICP(torch.nn.Module):
    """
    Differentiable Batched Iterative Closest Point for PyTorch
    
    This module aligns batches of source point clouds to target point clouds
    using the ICP algorithm. Supports point clouds with varying sizes.
    
    Implements various optimizations:
    - Grid acceleration for faster nearest neighbor search
    - CUDA streams for concurrent batch processing
    - Combined optimizations for maximum performance
    """
    
    def __init__(self, max_iterations=20, convergence_threshold=1e-6, 
                 use_grid_acceleration=True, use_cuda_streams=True, grid_cell_size=0.2):
        """
        Initialize the ICP alignment module.
        
        Parameters:
        -----------
        max_iterations : int
            Maximum number of ICP iterations
        convergence_threshold : float
            Threshold to determine convergence (change in error)
        use_grid_acceleration : bool
            Whether to use grid-based acceleration for nearest neighbor search
        use_cuda_streams : bool
            Whether to use CUDA streams for concurrent batch processing
        grid_cell_size : float
            Cell size for grid acceleration (only used if grid acceleration is enabled)
        """
        super(BatchedICP, self).__init__()
        # ...
    
    def forward(self, src_points, tgt_points, batch_indices=None):
        """
        Aligns batches of source point clouds to target point clouds using ICP
        
        Parameters:
        -----------
        src_points : torch.Tensor 
            Source points with shape [N, 3] or batched points [B, N, 3]
        tgt_points : torch.Tensor
            Target points with shape [M, 3] or batched points [B, M, 3]
        batch_indices : torch.Tensor, optional
            Batch indices for each point, shape [N] for src and [M] for tgt
            Only needed if not using batched points format
            
        Returns:
        --------
        aligned_points : torch.Tensor
            Aligned source points with same shape as src_points
        transforms : tuple(torch.Tensor, torch.Tensor)
            (rotation_matrices, translation_vectors)
            rotation_matrices: shape [B, 3, 3]
            translation_vectors: shape [B, 3]
        """
```

### BatchedChamferLoss

```python
class BatchedChamferLoss(torch.nn.Module):
    """
    Differentiable Batched Chamfer Distance Loss for PyTorch
    
    This module computes the Chamfer distance between batches of point clouds.
    Supports point clouds with varying sizes.
    """
    
    def __init__(self, reduction='mean', use_grid_acceleration=True, grid_cell_size=0.2):
        """
        Initialize the Chamfer distance loss module.
        
        Parameters:
        -----------
        reduction : str
            How to reduce the loss: 'mean', 'sum', or 'none'
        use_grid_acceleration : bool
            Whether to use grid-based acceleration for nearest neighbor search
        grid_cell_size : float
            Cell size for grid acceleration (only used if grid acceleration is enabled)
        """
        super(BatchedChamferLoss, self).__init__()
        # ...
    
    def forward(self, src_points, tgt_points, batch_indices=None, weights=None):
        """
        Computes Chamfer distance between batches of point clouds
        
        Parameters:
        -----------
        src_points : torch.Tensor 
            Source points with shape [N, 3] or batched points [B, N, 3]
        tgt_points : torch.Tensor
            Target points with shape [M, 3] or batched points [B, M, 3]
        batch_indices : torch.Tensor, optional
            Batch indices for each point, shape [N] for src and [M] for tgt
            Only needed if not using batched points format
        weights : torch.Tensor, optional
            Weights for each batch item, shape [B]
            
        Returns:
        --------
        loss : torch.Tensor
            Chamfer distance loss, scalar if reduction is 'mean' or 'sum',
            tensor of shape [B] if reduction is 'none'
        """
```

## Performance Optimizations

RapidAlign implements several optimization strategies to achieve maximum performance across different workloads:

### Grid-Based Acceleration

Grid-based acceleration partitions the 3D space into uniform cells to drastically speed up nearest neighbor searches:

- **Implementation**: 3D uniform grid with configurable cell size
- **Complexity Reduction**: Reduces nearest neighbor search from O(nÂ²) to approximately O(n)
- **Configuration**: Adjust `grid_cell_size` for optimal performance based on point cloud density

```python
# Enable grid-based acceleration
icp = BatchedICP(use_grid_acceleration=True, grid_cell_size=0.2)
```

The `grid_cell_size` parameter controls the trade-off between speed and accuracy. Smaller cells provide more accurate results but may be slower for sparse point clouds. Larger cells are faster but might miss some nearest neighbors.

Typical speedups: **3-8x** faster than brute force nearest neighbor search.

### CUDA Streams

CUDA streams enable concurrent execution of operations for different batch elements:

- **Implementation**: Separate CUDA stream for each point cloud in the batch
- **Benefit**: Significantly improved GPU utilization with multiple independent workloads
- **Use Case**: Most effective when processing large batches of point clouds

```python
# Enable CUDA streams for concurrent batch processing
icp = BatchedICP(use_cuda_streams=True)
```

Typical speedups: **1-1.2x** for single item batches, **3-5x** for large batches (16+ items).

### Combined Optimizations

For maximum performance, especially with large batches, you can enable both optimizations:

```python
# Enable all optimizations
icp = BatchedICP(
    use_grid_acceleration=True,
    use_cuda_streams=True,
    grid_cell_size=0.2
)
```

Typical speedups with combined optimizations: **9-30x** for large batches with many points.

## Advanced Usage

### Variable-Sized Batches

One of RapidAlign's key features is support for batches with varying point cloud sizes:

```python
import torch
from rapidalign import BatchedICP

# Create point clouds with different sizes
src_points_1 = torch.rand(1000, 3).cuda()
src_points_2 = torch.rand(1500, 3).cuda()
src_points_3 = torch.rand(800, 3).cuda()

tgt_points_1 = torch.rand(1000, 3).cuda()
tgt_points_2 = torch.rand(1500, 3).cuda()
tgt_points_3 = torch.rand(800, 3).cuda()

# Concatenate points
src_points = torch.cat([src_points_1, src_points_2, src_points_3], dim=0)
tgt_points = torch.cat([tgt_points_1, tgt_points_2, tgt_points_3], dim=0)

# Create batch indices
src_batch_idx = torch.cat([
    torch.zeros(1000, dtype=torch.int64),
    torch.ones(1500, dtype=torch.int64),
    torch.ones(800, dtype=torch.int64) * 2
]).cuda()

tgt_batch_idx = torch.cat([
    torch.zeros(1000, dtype=torch.int64),
    torch.ones(1500, dtype=torch.int64),
    torch.ones(800, dtype=torch.int64) * 2
]).cuda()

# Align variable-sized batches
icp = BatchedICP()
aligned_points, transforms = icp(src_points, tgt_points, 
                                (src_batch_idx, tgt_batch_idx))
```

### PyTorch Geometric Integration

RapidAlign seamlessly integrates with PyTorch Geometric for graph learning tasks:

```python
import torch
from torch_geometric.data import Data, Batch
from rapidalign import BatchedICP, BatchedChamferLoss

# Create PyG data objects
data1 = Data(
    x=torch.randn(10, 16),  # Node features
    pos=torch.randn(10, 3),  # Node positions
    edge_index=torch.randint(0, 10, (2, 20))  # Edges
)

data2 = Data(
    x=torch.randn(15, 16),
    pos=torch.randn(15, 3),
    edge_index=torch.randint(0, 15, (2, 30))
)

# Create a batch
batch = Batch.from_data_list([data1, data2])

# Extract positions and batch indices
pos = batch.pos.cuda()
batch_idx = batch.batch.cuda()

# Run alignment
target_pos = pos + torch.randn_like(pos) * 0.1
icp = BatchedICP()
aligned_pos, transforms = icp(pos, target_pos, batch_idx)

# Compute loss
chamfer_loss = BatchedChamferLoss()
loss = chamfer_loss(aligned_pos, target_pos, batch_idx)
```

### Custom GNN with Alignment

Example of a custom Graph Neural Network that incorporates point cloud alignment:

```python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from rapidalign import BatchedICP, BatchedChamferLoss

class SpatialGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(SpatialGNN, self).__init__()
        
        # GNN layers
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, out_channels)
        
        # Spatial transformation layer
        self.transform = torch.nn.Linear(out_channels, 3)
        
        # Alignment module
        self.alignment = BatchedICP(
            max_iterations=10,
            use_grid_acceleration=True,
            use_cuda_streams=True
        )
        
        # Chamfer loss for alignment
        self.loss_fn = BatchedChamferLoss(reduction='mean')
    
    def forward(self, x, edge_index, pos, batch=None):
        # If batch is None, assume a single graph
        if batch is None:
            batch = torch.zeros(x.size(0), device=x.device, dtype=torch.int64)
        
        # Message passing
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.conv3(x, edge_index)
        
        # Transform node positions based on features
        pos_offset = self.transform(x)
        transformed_pos = pos + pos_offset
        
        # Align transformed positions with original
        aligned_pos, _ = self.alignment(transformed_pos, pos, batch)
        
        # Compute alignment loss
        alignment_loss = self.loss_fn(aligned_pos, pos, batch)
        
        return x, aligned_pos, alignment_loss
```

## Benchmarks

RapidAlign has been extensively benchmarked across different workloads to evaluate the effectiveness of its optimization strategies:

### Single Point Cloud Alignment

Performance for a single point cloud with varying numbers of points:

| Points | Baseline | Grid Accel. | Speedup |
|--------|----------|-------------|---------|
| 1,000  | 15 ms    | 5 ms        | 3.0x    |
| 5,000  | 135 ms   | 20 ms       | 6.8x    |
| 10,000 | 520 ms   | 65 ms       | 8.0x    |
| 50,000 | 12,500 ms| 1,600 ms    | 7.8x    |

### Batch Processing Performance

Performance for batched alignment with 10,000 points per cloud:

| Batch Size | Baseline | CUDA Streams | Grid Accel. | Both | Max Speedup |
|------------|----------|--------------|-------------|------|-------------|
| 1          | 520 ms   | 510 ms       | 65 ms       | 64 ms| 8.1x        |
| 4          | 2,080 ms | 770 ms       | 260 ms      | 98 ms| 21.2x       |
| 16         | 8,320 ms | 1,910 ms     | 1,040 ms    | 275 ms| 30.3x      |

### Grid Cell Size Impact

Impact of grid cell size on performance and accuracy for 10,000 points:

| Cell Size | Search Time | Total Time | Avg Error | Notes |
|-----------|-------------|------------|-----------|-------|
| 0.05      | 22 ms       | 85 ms      | 0.00012   | Most accurate, slower |
| 0.10      | 14 ms       | 75 ms      | 0.00015   | Good balance |
| 0.20      | 9 ms        | 65 ms      | 0.00022   | Fastest, less precise |
| 0.50      | 5 ms        | 60 ms      | 0.00045   | Potential missing neighbors |

## Integration Examples

### Training Loop Example

```python
import torch
from torch_geometric.loader import DataLoader
from rapidalign import BatchedChamferLoss

# Assume we have a model and dataset
model = SpatialGNN(in_channels=16, hidden_channels=32, out_channels=32).cuda()
loader = DataLoader(dataset, batch_size=16, shuffle=True)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
model.train()
for epoch in range(100):
    total_loss = 0
    
    for data in loader:
        data = data.to('cuda')
        optimizer.zero_grad()
        
        # Forward pass with alignment
        node_features, aligned_pos, alignment_loss = model(
            data.x, data.edge_index, data.pos, data.batch
        )
        
        # Task-specific loss (e.g., node classification)
        task_loss = F.cross_entropy(node_features, data.y)
        
        # Combined loss
        loss = task_loss + 0.1 * alignment_loss
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {total_loss / len(loader):.6f}")
```

### Point Cloud Visualization

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from rapidalign import BatchedICP

# Create or load point clouds
src_points = torch.rand(1000, 3).cuda()
tgt_points = torch.rand(1000, 3).cuda() + 0.5

# Align point clouds
icp = BatchedICP(use_grid_acceleration=True)
aligned_points, _ = icp(src_points, tgt_points)

# Visualize results
fig = plt.figure(figsize=(15, 5))

# Source point cloud
ax1 = fig.add_subplot(131, projection='3d')
src_np = src_points.cpu().numpy()
ax1.scatter(src_np[:, 0], src_np[:, 1], src_np[:, 2], c='blue', s=2)
ax1.set_title('Source Point Cloud')

# Target point cloud
ax2 = fig.add_subplot(132, projection='3d')
tgt_np = tgt_points.cpu().numpy()
ax2.scatter(tgt_np[:, 0], tgt_np[:, 1], tgt_np[:, 2], c='red', s=2)
ax2.set_title('Target Point Cloud')

# Aligned point cloud
ax3 = fig.add_subplot(133, projection='3d')
aligned_np = aligned_points.cpu().numpy()
ax3.scatter(aligned_np[:, 0], aligned_np[:, 1], aligned_np[:, 2], c='green', s=2, alpha=0.7)
ax3.scatter(tgt_np[:, 0], tgt_np[:, 1], tgt_np[:, 2], c='red', s=2, alpha=0.3)
ax3.set_title('Aligned (green) vs Target (red)')

plt.tight_layout()
plt.savefig('alignment_result.png', dpi=300)
plt.show()
```

## Troubleshooting

### CUDA Version Mismatch

**Symptom**: Error message about CUDA version mismatch during installation.

**Solution**: Use the environment variable to bypass version check:

```bash
TORCH_ALLOW_CUDA_VERSION_MISMATCH=1 pip install -e .
```

### Out of Memory Errors

**Symptom**: CUDA out of memory errors during alignment of large point clouds.

**Solution**:
1. Reduce batch size
2. Use grid acceleration with larger cell size
3. Process in chunks if possible

```python
# Use larger grid cell size for memory efficiency
icp = BatchedICP(use_grid_acceleration=True, grid_cell_size=0.3)
```

### Poor Convergence

**Symptom**: ICP alignment not converging to good solutions.

**Solution**:
1. Increase maximum iterations
2. Use smaller convergence threshold
3. Ensure point clouds are pre-processed properly (e.g., normalization)

```python
# More iterations and tighter convergence
icp = BatchedICP(max_iterations=50, convergence_threshold=1e-7)
```

### Slow Performance

**Symptom**: Alignment operations taking longer than expected.

**Solution**:
1. Ensure you're running on GPU (`tensor.cuda()`)
2. Enable grid acceleration
3. For batched operations, enable CUDA streams
4. Optimize grid cell size for your data

## Contributing

We welcome contributions to RapidAlign! Please follow these steps to contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run the tests (`python -m pytest tests/`)
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Building Documentation

```bash
cd docs
pip install -r requirements.txt
sphinx-build -b html . _build/html
```

### Running Tests

```bash
# Run all tests
python -m pytest tests/

# Run performance tests
python -m pytest tests/test_performance.py

# Run functionality tests
python -m pytest tests/test_functionality.py
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.